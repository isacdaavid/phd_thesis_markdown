# Previous Research

## The neural bases of emotion and attention

Our modern understanding of emotional processing encompasses several
cortical and subcortical regions, some of which are rather primitive
and well-conserved. Roughly speaking, stimuli entering the thalamus is
sent to primary sensory cortices but also to the amygdala. The latter
possesses specialized nuclei for both innate and learned emotional
assessment. A long history of conditioning research has shown that the
amygdala must be present for implicit (i.e. procedural) associations
to be learned between emotional and unconditioned stimuli. The
amygdala's central nucleus projects to places such as the central gray
region of the brain stem and hypothalamus, where it can trigger
autonomic reactions and humoral activity. Cortical areas are also
implicated in complex emotional perception: the ventral anterior
cingulate cortex, insula and ventromedial prefrontal cortex. As an
example of the consequences these structures can have on emotion
perception, damage to the prefrontal cortex will result in sociopathic
impairments. [@ledoux-damasio-2013].

Discoveries in brain-damage patients, imaging and stimulation studies
have established the segregation of two distinct systems of visual
attention in humans, spanning, respectively, dorsal and ventral loci
at the frontoparietal cortex [@vossel-et-al-2014;
@umarova-et-al-2009].[^what-vs-where] The former is responsible for
goal-directed (top-down) attention, whereas the latter reacts to
unexpected (bottom-up) stimuli. It has also been suggested that they
serve as generic attentional mechanisms irrespective of sensory
modality [@macaluso-2010], or even in the absence of conceptual and
integrative information (e.g., during a feature-based selection task)
[@vandenberghe-gillebert-2009].

[^what-vs-where]: Not to be confused with the also dorsal ("where")
and ventral ("what") visual perception pathways. Although overlap with
attention networks may exist at the temporoparietal junction and
intraparietal sulcus.

Inquiries into the functional, causal and structural connectivity of
dorsal and ventral networks have begun to shed light into their
components, and — to some degree — their specialization. The dorsal
network comprises at the very least the frontal eye fields (FEF) and
intraparietal sulcus (IPS), both of which are thought to display
association tracts to the retinotopically and
contralaterally-organized perceptual areas in the occipital lobe, plus
commissure fibers to connect heminetworks with each other. It's
possible that the dorsal network computes planned saliency maps, as
suggested by the presence of even more retinotopically-organized
receptive fields in FEF and IPS. Saliency maps would then be consumed
by the FEF to request all many sorts of eye movements
[@jerde-et-al-2012].

The inner workings of the ventral frontoparietal network are more
poorly understood, but it has been reliably associated with the
filtering of exogenous (i.e. task-irrelevant) events, presumably
allowing unexpected and potentially dangerous stimuli to overcome the
subject's concentration and become the focus of attention. Activation
occurs at zones dubbed the temporoparietal junction (TPJ) and the
ventral frontal cortex (VFC). Issues exist determining whether the
ventral network is right-lateralized, and no cytoarchitectonic (or
otherwise) standard exists setting boundaries for the TPJ and VFC
[@vossel-et-al-2014].

The TPJ has been suggested as a point of interaction between systems
[@corbetta-et-al-2008; @fox-et-al-2006]. Intrinsic activity in the
right posterior middle frontal gyrus (MFG) has been correlated with
both networks, making it an additional candidate hub. Of all three
major superior longitudinal fasciculi (SFL I, II and III) providing
associative anatomical connections for these networks, the middle one
(SFL II) is known to connect FEF and TPJ [@de-schotten-et-all-2011].

## For and against automatic emotion perception

The affective priming paradigm [@klauer-musch-2003] is known to
improve reaction times or accuracy for subsequent identification of
identical emotions, thereby reflecting some versatility in the
processes leading to affect recognition [@de-houwer-2009]. Many lines
of evidence help extrapolate this fact into the notion that affect
perception can undergo an automatic/preattentional mode:

- Because affective priming occurs only under short stimulus onset
  asynchrony (_SOA_) times[^soa] (300 ms or less), it has been
  conjectured that its processing must occur before the direction of
  attention and response strategies take place [@moors-de-houwer-2006;
  @hermans-et-al-2001].

[^soa]: The time lapse between prime and target stimuli presentation.

- The effect is observed even when the prime is presented at
  unrecognizable subthreshold levels [@draine-greenwald-1998] and
  outside the focus of visual attention [@calvo-nummenmaa-2007].

- According to some reports, cognitive load (as produced by the
  simultaneous presentation of irrelevant tasks) does not impair
  affective priming [@hermans-et-al-2000]. This is disputed, though.

A further refinement proposes that not all emotions were made
equally advantageous to recognize. If natural selection produced
complementary high-priority neural circuitry for the processing of
facial expressions (or an epigenetically-developed precursor), one
could naturally ask whether this automatism also targeted some
emotions more than others based on survival advantage. Indeed, a
wealth of research <!-- mention cognitive bias towards type 1 error
during threat detection?  --> has found that the priming effect, as
well as others, are biased towards averse and pejorative emotions as
opposed to happy or neutral ones [@fox-et-al-2002; @vuilleumier-2001;
@ishai-et-al-2004; @vuilleumier-2005; @susa-et-al-2012].

Contrary to the previous view, @pessoa-et-al-2002 interpreted the
increased activity in cortical and basal structures linked to facial
recognition and emotion (fusiform gyrus, amygdala, etc.) during
explicit attention to facial features (in contrast to non-expressive
details inscribed in the faces) as evidence that facial processing is
contingent upon attention. This strand of facial emotion processing
research is not without more recent supporters [see
@ochsner-gross-2005; @eimer-et-al-2003 for instance]. EEG[^eeg]
studies attempting to correlate alleged ERP[^erp] indices of visual
attention with different modes of emotion perception have thrown mixed
results so far (see @galfano-et-all-2011 for a review and negative
results concerning the emotion-dependent hypothesis, as inferred from
2nd-negative signal components at posterior contralateral electrodes
during a gaze-cuing task).

[^eeg]: Electroencephalography.
[^erp]: Event-related potential. Sometimes used interchangeably with
"evoked potential"

Seemingly contradictory results stemming from methodologically sound
studies cry for a theoretical reformulation to encompass all the
facts. Research around the conflict often resorts to some sort of
interaction and modulation between systems to explain the data
[@okon-et-al-2007; @palermo-rhodes-2007]. Based on reaction times and
varying task difficulty, Sassi and colleges proposed that even though
emotional perception can be turned automatic, spare resources might
still be consumed in parallel; should the distracting task not be
distracting enough [@sassi-et-al-2014].
